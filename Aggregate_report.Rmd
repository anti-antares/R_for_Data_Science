---
title: "Final project: Costa Rican Household Poverty Level Prediction"
author: "Zhexin Chen, Shuting You and Xiaodi Tao"
date: October 19, 2018
output: html_notebook
---

This is the final project for CMU 95_778 R for Data Science. The kaggle competition we choose is [Costa Rican Household Poverty Level Prediction](https://www.kaggle.com/c/costa-rican-household-poverty-prediction) held by the Intra-American Development Bank and Kaggle. The project GitLab repository is available [here](https://gitlab.com/antileo1992/95_778.final_project).


##Business Problem Overview: <br/>
Inter-American Development Bank is working on a social program to aid impoverished individuals in Costa Rica. Impoverished individuals are usually unlikely to have direct financial documents(income or expenses) to prove that they truly need the aid. Current models (PMT), which take observable household attributes into account (eg.how many rooms in a house? how many households? )  still needs improvement in terms of prediction accuracy. Our team is working on solving these challenges and helping Inter-American Development Bank identify the most impoverished individuals to give aid to. 


#The business questions we aim to address in our project are: <br/>
What are the most important variables that goes into identifying household poverty level  
What model to recommend to improve household poverty identification accuracy for the Inter-American Development Bank to offer aiding programs to the people that are most in need. 


```{r warning=FALSE,message=FALSE}
library(tidyverse)
library(caret)
library(ggplot2)
library(randomForest)
library(scales)
library(reshape2)
library(e1071)
library(DataExplorer)
library(corrplot)
library(gridExtra)
library(nnet)
library(factoextra)
library(gbm)

poverty=read_csv("Train.csv")
poverty_test=read_csv("test.csv")
```


##Explore and Clean Data


```{r}
introduce(poverty)
```

```{r}
introduce(poverty_test)
```

The datasets contain 142 features and 1 label (only for the training set). The datasets give comprehensive description of a household surveyed, and the features contain information about several aspects of important information of a household: including household finance, household items, house condition, family composition, education, and basic demographics. 

Below is a list of some representative features:

Household Finance:
  v2a1, Monthly rent payment
  ...
  
Household Items:
  v18q1, number of tablets household owns
  refrig, =1 if the household has refrigerator
  ...
  
House Condition:
  rooms,  number of all rooms in the house
  paredblolad, =1 if predominant material on the outside wall is block or brick
  ...
  
Family Composition:
  hhsize, household size
  tamviv, number of persons living in the household
  hogar_adul, Number of adults in household
  ...
  
Education:
  instlevel1, =1 no level of education
  ...
  
```{r}
table(poverty$Target)
```


```{r}
ggplot(poverty)+geom_bar(aes(Target))+ggtitle("Distribution of Label")+theme(plot.title = element_text(hjust = 0.5))+scale_y_continuous(labels = comma)
```


The label "Target" has 4 levels, coded as: "1", "2", "3" and "4", where “1” stands for extreme poverty and "4" stands for non vulnerable households.

The training set has 9557 observations and the testing set has 23856 observations. In order to obtain a higher accuracy on the testing set, on the one hand, our model should include as much information as possible, and on the other hand, we should restrict our model to be overfitting.


#Missing Plot
```{r}
poverty_missing=poverty %>%  select(everything()) %>% summarise_all(funs(sum(is.na(.)))) %>% gather(variables,missing_amount) %>% mutate(percent=missing_amount/nrow(poverty)) %>% 
  arrange(percent) %>% mutate(levels=case_when(percent>=0.8~"Remove",
                                              percent>=0.4 & percent<0.8~"Bad",
                                              percent>=0.05 & percent<0.4~"OK",
                                              percent<0.05~"Good")) %>% filter(missing_amount>0)

ggplot(poverty_missing, aes(x = reorder(variables, -percent), y = missing_amount,fill=levels)) + geom_bar(stat = "identity") + coord_flip()+ scale_fill_manual(values=c("#1a9641", "#a6d96a", "#d7191c"))+ xlab("Features") + ylab("Number of missing rows") +   ggtitle("Missing variable plot")+ 
  geom_text(aes(label = paste0(round(100 * percent, 2), "%"))) + scale_y_continuous(labels = comma)+theme(plot.title = element_text(hjust = 0.5))
```

From the missing plot, we can see there appear NAs in 5 variables and 3 of them have a large proportion of NAs, therefore on the later stage, we tend to drop these three variables and replace NAs for the remaining two.

By the way, we find the data has already been dummified, like below:
lugar1, =1 region Central
lugar2, =1 region Chorotega
lugar3, =1 region PacÃƒÂfico central
lugar4, =1 region Brunca
lugar5, =1 region Huetar AtlÃƒÂ¡ntica
lugar6, =1 region Huetar Norte

Therefore in order to reduce the time for training some certain models (for example, random forest), we may reconstruct these features to make reduce the dimension of features.
#Features Distribution Analysis 

We want to see how household variables that discribe different perspectives of household are distributed in our model. Some are normally distirbuted, some are left-skewed, and some do not possess certain patterns. And we use different colors to differenciate different levels of poverty.(with 1 being the extremely poor individuals, with 4 being the non vulnerables)

The variable that represent number of all rooms in the house are normally distributed. <br/>
The variable that represent size of the household and number of mobile phones are left-skewed.<br/>
The variable that represent years of years of schooling do not possess a certain pattern. 

Within each distribution, we can see that the non vulnerables group takes up most percentanges. This is due to the fact that our datasets naturally contains more non vulnerables. 


Later in the Pinciple Component Analysis Section, we will choose 10 prinicipal components that explain the variances most. Thus, we would not bother to transform the data for our multinomial logistics model in our project.
```{r}
a<-ggplot(poverty,aes(x=rooms))+geom_bar(aes(fill=as.factor(Target)))+ylab("Count")+xlab("Number of All Rooms in the House")+labs(fill="Poverty")

b<-ggplot(poverty,aes(x=tamhog))+geom_bar(aes(fill=as.factor(Target)))+ylab("Count")+xlab("Size of the Household")+labs(fill="Poverty")

c<-ggplot(poverty,aes(x=escolari))+geom_bar(aes(fill=as.factor(Target)))+ylab("Count")+xlab("Years of Schooling")+labs(fill="Poverty")

d<-ggplot(poverty,aes(x=qmobilephone))+geom_bar(aes(fill=as.factor(Target)))+ylab("Count")+xlab("Number of Mobile Phones")+labs(fill="Poverty")

grid.arrange(a,b,c,d, nrow = 2,top="Variables Distribution Analysis")
```

Let's see among the 142 variables, which variables have the high relations with the degree of poverty (described as the variable called "Target"，the bigger the number, the less of the degree of poverty). For the first step, we can pick some variables to draw precise boxplots and scatterplots and we notice that years of schooling (described as the variable called escolari), and degree of crowding in the house (described as the variable called overcrowding) can have some trends with degree of poverty (some features of these variables change as the degree of poverty goes up). Thus, they should be highly focused when doing the model.
```{r}
p1 <- ggplot(poverty, aes(x = as.factor(Target), y = rooms)) +
  geom_boxplot(aes(fill=as.factor(Target))) +
  labs(fill="degree_of_poverty")+
  coord_flip()+
  labs( x="degree of poverty")+
  ggtitle("Frequency distribution - number of rooms in house") +
  theme(plot.title = element_text(hjust = 0.5, size = 10))

p2 <- ggplot(poverty, aes(x = as.factor(Target), y = hogar_total)) +
  geom_boxplot(aes(fill=as.factor(Target))) +
  labs(fill="degree_of_poverty")+
  coord_flip()+
  labs( x="degree of poverty")+
  ggtitle("Frequency distribution - number of total individual in household") +
  theme(plot.title = element_text(hjust = 0.5, size = 10))

grid.arrange(p1,p2,nrow=2)
```

```{r}
p3 <- ggplot(poverty, aes(x = as.factor(Target), y = SQBdependency)) +
  geom_boxplot(aes(fill=as.factor(Target))) +
  labs(fill="degree_of_poverty")+
  coord_flip()+
  labs( x="degree of poverty")+
  ggtitle("Frequency distribution - dependency squared") +
  theme(plot.title = element_text(hjust = 0.5))

p4 <- ggplot(poverty, aes(x = as.factor(Target), y = escolari)) +
  geom_boxplot(aes(fill=as.factor(Target))) +
  labs(fill="degree_of_poverty")+
  coord_flip()+
  labs( x="degree of poverty")+
  ggtitle("Frequency distribution - year of schooling") +
  theme(plot.title = element_text(hjust = 0.5))

p5 <- ggplot(poverty, aes(x = as.factor(Target), y = overcrowding)) +
  geom_boxplot(aes(fill=as.factor(Target))) +
  labs(fill="degree_of_poverty")+
  coord_flip()+
  labs( x="degree of poverty")+
  ggtitle("Frequency distribution - degree of overcrowding") +
  theme(plot.title = element_text(hjust = 0.5))
grid.arrange(p3,p4,p5,nrow=3)
```

Then we try to use scatterplot to analyze the relationship between variables, we tried a lot and found out that “tamhog (size of the household)” and “tamviv (number of persons living in the household)”, “r4t2 (persons 12 years of age and older)” and “hogar_adul (Number of adults in household)” have a strong positive relationship which can be seen in the following scatterplots. Thus, when doing the model, we should consider relationships between variables such as these ones.
```{r}
p6 <- ggplot(poverty, aes(x = tamhog, y = tamviv,color=as.factor(Target))) +
  geom_point(alpha = 0.3) +
  labs(color="degree_of_poverty")+
  coord_flip()+
  ggtitle("Scatterplot -relationship_tamhog_tamviv") +
  theme(plot.title = element_text(hjust = 0.5))

p7 <- ggplot(poverty, aes(x = hogar_adul, y = r4t2,color=as.factor(Target))) +
  geom_point(alpha = 0.3) +
  labs(color="degree_of_poverty")+
  coord_flip()+
  ggtitle("Scatterplot -relationship_r4t2_hogar_adul") +
  theme(plot.title = element_text(hjust = 0.5))
grid.arrange(p6,p7,nrow=2)
```
#Data Clean
```{r}

feature_list = c(
  "pared",
  "piso",
  "techo",
  "abasta",
  "sanitario",
  "energcocinar",
  "elimbasu",
  "epared",
  "etecho",
  "eviv",
  "estadocivil",
  "parentesco",
  "instlevel",
  "tipovivi",
  "lugar",
  "area"
)


new_features_integer = data.frame(matrix(ncol = length(feature_list), nrow = nrow(poverty)))

ohe_names = vector()

for(i in 1:length(feature_list)){
  
  feature_to_fix = poverty %>% select(starts_with(feature_list[i]))

  
  new_features_integer[,i] = as.integer(factor(names(feature_to_fix)[max.col(feature_to_fix)], ordered = FALSE))
  names(new_features_integer)[i] = paste0(feature_list[i],"_int")
  
  ohe_names = c(ohe_names, as.vector(names(feature_to_fix)))
  
}
for(i in 1:length(feature_list)){
  poverty =poverty %>% select(-starts_with(feature_list[i]))
}

poverty=as_tibble(cbind(poverty , new_features_integer))

new_features_integer = data.frame(matrix(ncol = length(feature_list), nrow = nrow(poverty_test)))

for(i in 1:length(feature_list)){
  

  feature_to_fix = poverty_test %>% select(starts_with(feature_list[i]))
  
 
  new_features_integer[,i] = as.integer(factor(names(feature_to_fix)[max.col(feature_to_fix)], ordered = FALSE))
  names(new_features_integer)[i] = paste0(feature_list[i],"_int")
  
  ohe_names = c(ohe_names, as.vector(names(feature_to_fix)))
  
}
for(i in 1:length(feature_list)){
  poverty_test =poverty_test %>% select(-starts_with(feature_list[i]))
}

poverty_test=as_tibble(cbind(poverty_test , new_features_integer))
```

##Correlation
```{r}
corrma=round(cor(poverty %>% select(-v2a1,-v18q1,-rez_esc,-dependency,-edjefa) %>% na.omit()%>% select_if(is.numeric)),2)

corrplot(corrma,order = "hclust", tl.cex = 0.5)
```

##Correlation
Following is the correlation plot of all the 142 variables. For each pair of the variables, we can see the correlation. If the color is darker, it indicates that the the degree of correlation is deeper. Blue means the positive correlation and red means the negative correlation. Notice that some of the variables such as age and SQBage is highly related because SQBage is age squared. Thus, when building the model, we should deal with it. For example, we can choose one from SQBage and age since they carry similar information.

##PCA

PCA

The most important vairable (PC1) and second most important variable(PC2) explain 16.8% of the variances of the dataset in total. 


From the graph, We want to interpret these two variables:


PC1 is largely related with numnber of people in the households and how crowded the home is. 
Some contributing variables are: 
hogar_total, # of total individuals in the household
r4t3, Total persons in the household
hhsize, household size
hogar_nin, Number of children 0 to 19 in household
overcrowding  # persons per room


PC2 is largely related with education: 
Some contributing variables are: 
esolari, years of schooling
SQBescolari, escolari squared

The first 10 principal components chosen by the PCA explain 36.9% of the dataset variances, which is pretty significant in our model. 
```{r warning=FALSE,message=FALSE}

poverty_pca <- read_csv(file = "Train.csv")
poverty_pca_test <-read_csv(file ="test.csv")


poverty_pca$tag <-"Train"
poverty_pca_test$tag <-"Test"

poverty_pca_test $Target <-1
poverty_pca_total <- rbind(poverty_pca,poverty_pca_test)
poverty_pca_total <- poverty_pca_total %>% mutate(Target=as.factor(Target))
poverty_pca_total$v2a1 <-NULL  #NA
poverty_pca_total$v18q1 <-NULL  #NA
poverty_pca_total$rez_esc <-NULL #NA   #33413
poverty_pca_total$meaneduc <-NULL
poverty_pca_total$SQBmeaned <-NULL

poverty_pca_total$dependency <- NULL
poverty_pca_total$idhogar <-NULL

full = as_tibble(rbind(poverty_pca %>% select,poverty_pca_test))

# Create a list of the features names that need to be reverse engineered

feature_list = c(
  "pared",
  "piso",
  "techo",
  "abasta",
  "sanitario",
  "energcocinar",
  "elimbasu",
  "epared",
  "etecho",
  "eviv",
  "estadocivil",
  "parentesco",
  "instlevel",
  "tipovivi",
  "lugar",
  "area"
)

new_features_integer = data.frame(matrix(ncol = length(feature_list), nrow = nrow(full)))

ohe_names = vector()

for(i in 1:length(feature_list)){
  
  feature_to_fix = full %>% select(starts_with(feature_list[i]))
  
  new_features_integer[,i] = as.integer(factor(names(feature_to_fix)[max.col(feature_to_fix)], ordered = FALSE))
  names(new_features_integer)[i] = paste0(feature_list[i],"_int")
  
  ohe_names = c(ohe_names, as.vector(names(feature_to_fix)))
  
}

poverty_pca_numerical = poverty_pca_total %>% select_if(is.numeric)  ## get the numerical columns 
poverty_pca_numerical$elimbasu5 <- NULL

full_pca = prcomp(poverty_pca_numerical, center = TRUE, scale. = TRUE)
```

```{r}
fviz_eig(full_pca, addlabels=TRUE)
```

```{r}


fviz_pca_var(full_pca,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             select.var = list(contrib = 20), # top 20 contributing
             repel = TRUE,     # Avoid text overlapping
             title="PCA-Plot")

poverty_pca_total= poverty_pca_total %>% 
  cbind(full_pca$x[,1:10])

poverty_pca_train <- subset(poverty_pca_total, tag=="Train")
poverty_pca_test <- subset(poverty_pca_total, tag=="Test")
poverty_pca_test$Target <-NULL

```

#Data Preparation
```{r warning=FALSE, message=FALSE}
#randomForest
impute.zero <- function(x) replace(x, is.na(x), 0)
poverty_rf=poverty %>% select(-v2a1,-v18q1,-rez_esc,-Id,-idhogar,-dependency,-edjefa) %>% mutate(SQBmeaned=impute.zero(SQBmeaned),
                                                                                         meaneduc=impute.zero(meaneduc))
poverty_rf$Target=as.factor(poverty_rf$Target)

#GBM
poverty_gbm <- read_csv("Train.csv") %>%
  mutate(Target = as.factor(Target)) %>% 
  select(-dependency,-Id,-idhogar,-v2a1,-v18q1,-rez_esc,-SQBmeaned,-meaneduc)  

poverty_test_gbm <- poverty_test %>%
  select(-dependency,-idhogar,-v2a1,-v18q1,-rez_esc,-SQBmeaned,-meaneduc) 

#Data Split
seed <- 100
set.seed(seed)

inTraining <- createDataPartition(poverty_rf$Target, p=0.7, list=FALSE)
poverty_rf_training=poverty_rf[inTraining,]
poverty_rf_validation=poverty_rf[-inTraining,]

inTraining <- createDataPartition(poverty$Target, p=0.7, list=FALSE)
poverty_gbm_training=poverty_gbm[inTraining,]
poverty_gbm_validation=poverty_gbm[-inTraining,]

inTraining <-createDataPartition(poverty_pca_train$Target,p=0.7,list=FALSE)
poverty_pca_train_training = poverty_pca_train[inTraining,]
poverty_pca_train_validation = poverty_pca_train[-inTraining,]
```


#Random Forest
##Limitation

We splilt the training data into a training set (70%) and (30%), and replace the NAs with 0s. Then We first train the random forest model with tuneLength 5, and choose the mtry with highest accurary, that is, 75 for the following trainings. However, due to the limited computation resources, this time we are not able to conduct a more precise search of parameters and a cross validation to reduce overfitting.

```{r}
impute.zero <- function(x) replace(x, is.na(x), 0)
poverty_rf=poverty %>% select(-v2a1,-v18q1,-rez_esc,-Id,-idhogar,-dependency,-edjefa) %>% mutate(SQBmeaned=impute.zero(SQBmeaned),
                                                                                         meaneduc=impute.zero(meaneduc))
poverty_rf$Target=as.factor(poverty_rf$Target)

seed <- 100
set.seed(seed)

inTraining <- createDataPartition(poverty_rf$Target, p=0.7, list=FALSE)
poverty_rf_training=poverty_rf[inTraining,]
poverty_rf_validation=poverty_rf[-inTraining,]

#control <- trainControl(method="repeatedcv", number=10, repeats=3)
tTrace=trainControl(verboseIter = TRUE)

metric <- "Accuracy"

mtry <- 75
tunegrid <- expand.grid(.mtry=mtry)

#rf_fit3 <- train(Target ~ ., data = poverty_rf_training, method = "rf", metric = "Accuracy", tuneGrid=tunegrid,trControl=tTrace)

#print(rf_fit3)
#save(rf_fit3，file="rf_fit3.rda")



load("rf_fit3.rda")
pred_validation <- predict(rf_fit3, newdata=poverty_rf_validation)
confusionMatrix(data=pred_validation, poverty_rf_validation$Target)

poverty_rf_test=poverty_test%>% select(-v2a1,-v18q1,-rez_esc,-idhogar,-dependency,-edjefa) %>% mutate(SQBmeaned=impute.zero(SQBmeaned),
                                                                                         meaneduc=impute.zero(meaneduc))
pred <- predict(rf_fit3, newdata=poverty_rf_test)
result=data.frame(poverty_rf_test$Id,pred)
colnames(result)=c("Id","Target")
write.csv(result,"rf_submission.csv",row.names=FALSE)
```

From the confustion matrix we get from the prediction, the overal accuracy is pretty high, but the prediction accuracy is not balanced among the 4 levels. The prediction accuracy decreases when predicting the poorer class, although 89% accuracy is good enough.

For the extremley poor class (Class 1), the model has a sensitivity of 78.8%, which means the model correctly predicts 78.8% of all the times when the household is actually in the extremely poor class (Class 1).

```{r}
varImp(rf_fit3)
```


```{r}
impvars <- varImp(rf_fit3)
plot(impvars, main = "Variable Importance for Random Forest")
```


From the variable importance plot, we can see the top 2 important variables are all about education (average years of education for adults), then followed by dependency ratio, mobile phone amounts, and the predominant material on the outside. In the below box plots, we can find strong relationships between these features and the label.

```{r}
p8 <- ggplot(poverty_rf, aes(x = as.factor(Target), y = meaneduc)) +
  geom_boxplot(aes(fill=as.factor(Target))) +
  labs(fill="degree_of_poverty")+
  coord_flip()+
  labs( x="degree of poverty")+
  ggtitle("Frequency distribution - years of education") +
  theme(plot.title = element_text(hjust = 0.5))

p9 <- ggplot(poverty_rf, aes(x = as.factor(Target), y = SQBdependency)) +
  geom_boxplot(aes(fill=as.factor(Target))) +
  labs(fill="degree_of_poverty")+
  coord_flip()+
  labs( x="degree of poverty")+
  ggtitle("Frequency distribution - dependency ratio squared") +
  theme(plot.title = element_text(hjust = 0.5))

p10 <- ggplot(poverty_rf, aes(x = as.factor(Target), y = qmobilephone)) +
  geom_boxplot(aes(fill=as.factor(Target))) +
  labs(fill="degree_of_poverty")+
  coord_flip()+
  labs( x="degree of poverty")+
  ggtitle("Frequency distribution - number of mobile phones") +
  theme(plot.title = element_text(hjust = 0.5))
grid.arrange(p8,p9,p10,nrow=3)
```


#GBM
##Limitations:  
Since we have a limit background knowledge of the mathematical meaning behind the Gbm model, we don’t know much about how to adjust the parameters in the Gbm model. Thus, we didn’t adjust a lot to improve Gbm model, but merely try our best to run it.


##The process of doing the model:  
Since some of the variables have a lot of NA, and from the EDA process we found out that some of them such as v2a1, v18q1, rez_esc, SQBmeaned and meaneduc has too many NAs that we must delete those columns. Also, variables such as edjefa and idhogar also has some NAs and  we know that they are not so important to the degree of poverty based on EDA. Finally, Id has nothing related with the degree of poverty, so we delete all the variables mentioned above when doing Gbm model.


We saved the model as rda file in order to reuse it next time so we comment the code of the model as follow.

```{r}
#poverty_gbm <- expand.grid(interaction.depth = 5,
#                        n.trees = 100,
#                        shrinkage = 0.1,
#                        n.minobsinnode = 10)

#final_Gbm <- train(Target ~., data = poverty_gbm_training, method = "gbm",
#                   tuneGrid = poverty_gbm,
#                   verbose = TRUE)
poverty_gbm_test=read_csv("test.csv")%>%
  select(-dependency,-idhogar,-v2a1,-v18q1,-rez_esc,-SQBmeaned,-meaneduc) 
load("2final_Gbm_.rda")

varImp(final_Gbm)
```
##Findings:
##Variable importance:
From the variable importance plot, we can see that the top three important variables are SQBdependncy, cielorazo, and overcrowding. The above variables mean that dependency, the house quality, the number of children and the degree of overcrowding affect the degree of poverty a lot. Note that when we  did EDA and drew the boxplot, we did find out that overcrowding may be important variable, and the result in Gbm model really shows that we were right.


Also, note that from the boxplots below, we can know that SQBdependncy and overcrowding have the negative relationship with Target. In actual world, it makes sense because when people have more dependencies and are overcrowded when living, they tend to be poor. What’s more, when the quality of house is good, they tend not to be poor.
```{r}
impvars <- varImp(final_Gbm)
plot(impvars, main = "Variable Importance for GBM")

p11 <- ggplot(poverty_gbm_training, aes(x = as.factor(Target), y = overcrowding)) +
  geom_boxplot(aes(fill=as.factor(Target))) +
  labs(fill="degree_of_poverty")+
  coord_flip()+
  labs( x="degree of poverty")+
  ggtitle("Frequency distribution - number of persons per room") +
  theme(plot.title = element_text(hjust = 0.5))

p12 <- ggplot(poverty_gbm_training, aes(x = as.factor(Target), y = SQBdependency)) +
  geom_boxplot(aes(fill=as.factor(Target))) +
  labs(fill="degree_of_poverty")+
  coord_flip()+
  labs( x="degree of poverty")+
  ggtitle("Frequency distribution - dependency ratio squared") +
  theme(plot.title = element_text(hjust = 0.5))

p13 <- ggplot(poverty_gbm_training, aes(x = as.factor(Target), y = cielorazo)) +
  geom_boxplot(aes(fill=as.factor(Target))) +
  labs(fill="degree_of_poverty")+
  coord_flip()+
  labs( x="degree of poverty")+
  ggtitle("Frequency distribution - the area of the ceiling") +
  theme(plot.title = element_text(hjust = 0.5))
grid.arrange(p11,p12,p13,nrow=3)
```

##Confusion matrix:
From the confusion matrix, what we noticed is that the accuracy of Gbm model is 77.7%, not bad. As for the sensitivity, since our goal is to predict the poor people in the country, and the lower the number is, the poorer the people are, so number “1” means the poorest people. And our sensitivity for “1” is 48.61%, meaning that for for all the class 1 in the real world, 48.51% we predict is “1”, not very good. As for PPV, the PPV for “1” is 87.23%, meaning that for what we predict as "1", 87.23% of them is actually “1”.
```{r}
pred_Gbm <- predict(object = final_Gbm, newdata = poverty_gbm_validation, type = "raw")
confusionMatrix(pred_Gbm, poverty_gbm_validation$Target)

poverty_test_id<-poverty_test %>% 
  select(Id)

predTest_Gbm <- predict(object = final_Gbm, newdata = poverty_gbm_test, type = "raw")
result=data.frame(poverty_test_id,predTest_Gbm)
colnames(result)=c("Id","Target")
write.csv(result,"gbm_submission.csv",row.names=FALSE)

```
 

#Multinomial logistics regression


In the dataset we are working on, the dependent variables have 4 different levels, instead of 2. So we use the multinomial logistics regression model.  

[Confusion Matrix Interpretation]

In the validation dataset, the model has a overall accuracy of 0.6482. Out of the predictions of Extreme Poverty,Moderate Porverty, Vulnerable Households and Non Vulnerable Households, we got 64.82% of the predictions correctly. 

For the prediction of Extreme poverty(Class 1), the model has a sensitivity of 2.2%, a Pos Pred Value of 41.67%. This suggsts that

1) Out of individuals who are actually extremely poor, we successfully predict about 2.2% of the times. The model does not perform very well in terms of this metric.

2) Out of all the individuals that we predict to be extremely poor, 41.6% are actually extreme poor. 


[Coefficient analysis]

With PC1, it has a negative relationship with the poverty level. The coefficient PC1 of multinomial logistics is negative, suggesting that with PC1, individuals are more likely to be turning to the poorer level. Remember that in the EDA section, we discovered that PC1 is related with overcrowding of the household. That suggests that if household is more crowded, than the household is more likely on average to be poorer.

The coefficient PC2 of multinomial logistics regression is positive, suggesting that with PC2, individuals are more likely to be turning to the non vulnerable level. And again in the EDA section, we discovered that PC2 is related with years of schooling. That suggests that with more years of schooling, the household is more likely on average to be richer. 

All of the 2 coefficient analysis makes sense intuitively: When households are poorer, their homes are more likely to be crowded and they have more people in a home. When households are richer, they have longer years of schooling because they are able to afford the huge amount of tuitions and living costs. 

Again when we look back to the EDA: we are able to find similar patterns. 

```{r}
poverty_pca_multinolog <- multinom(Target ~PC1+PC2+PC3+PC4+PC5+PC6+PC7+PC8+PC9+PC10, data=poverty_pca_train_training)
summary(poverty_pca_multinolog)

poverty_pca_log_prediction_train <- predict(object = poverty_pca_multinolog, newdata = poverty_pca_train_training) 
## table(actual = poverty_pca_train$Target, predict = poverty_pca_log_prediction_train)
## confusionMatrix(poverty_pca_log_prediction_train, poverty_pca_train$Target)

poverty_pca_log_prediction_validation <- predict(object = poverty_pca_multinolog, newdata =poverty_pca_train_validation)
confusionMatrix(poverty_pca_log_prediction_validation, poverty_pca_train_validation$Target)

poverty_pca_log_prediction_test <- predict(object = poverty_pca_multinolog, newdata =poverty_pca_test)


result <-data.frame(poverty_pca_test$Id,poverty_pca_log_prediction_test)
colnames(result) <-c("Id","Target")
write.csv(result,"submission_shuting.csv",row.names = FALSE)

```


```{r}
p13 <- ggplot(poverty_pca_train_training, aes(x = as.factor(Target), y = PC1)) +
  geom_boxplot(aes(fill=as.factor(Target))) +
  labs(fill="degree_of_poverty")+
  coord_flip()+
  labs( x="degree of poverty")+
  ggtitle("Frequency distribution - principle component 1") +
  theme(plot.title = element_text(hjust = 0.5))
p13
#grid.arrange(p13,p14,nrow=2)
```



##[Business recommendations]
Based on our EDA, models and findings anlayses, we have the following recommendations in response to the business questions for the project. <br/>

**1) Recommend variables that are related to schooling, household crowding, house quality, and number of cellphones owned as the most important variables from both the models and EDA analysis. **<br/>

From the models we found that: <br>
More schooling generally indicates that households are less non vunlenerable. <br/>
schooling related variables include:<br/>
Meaneduc，the degree of education 

household crowding related variables: household being crowded generally indicates that households are more impoverished. <br/>
r4t3, Total persons in the household;<br/>
overcrowding  # persons per room 

house quality related variables: worse house quality generally indicates that households are more impoverished <br/>
Cielorazo, the area of the ceilin

number of cellpones owned: more number of cellphones owned generally indicates that households are less non impoverished.<br/>
Qmobilephone

These interpretations from the models are very similar to the ones we got from the boxplots we previously drawn. <br/>

These interpretations may seem to be very intuitive to what we usually think. But this helps us to identify the most important variables to look at when predicting poverty levels. Thus, they do provide some level of practical senses. 



**2) Recommend the random forest model for the Inter-American Development Bank to identify impoverished individuals for the bank to provide the aiding programs. **

<<<<<<< HEAD
As for the model overall accuracy, logistics regression has an accuracy of 64.82%, rf has an accuracy of 93.72%, Gbm rf has an accuracy of 77.7%.
When talking about sensitivity for Class 1, logistics regression has sensitivity of 2.21%, rf has sensitivity of 78.76%, Gbm has sensitivity of 48.62%.
As for PPV for Class 1, logistics regression has PPV of 41.67%, rf has PPV of 97.8%, Gbm has PPV of 87.23%.
When it goes to kappa, logistics regression has kappa of 0.1892, rf has kappa of 0.883, Gbm has kappa of 0.5473.
When it comes to the scores on Kaggle, logistics regression has 0.29, rf has 0.375, Gbm has 0.368.
Overall, we recommend the Random Forest Model, because it has a highest accuracy, highest sensitivity(Class1), highest PPV (Class1), highest Kappa, and highest Kaggle scores.
=======

##[Vision of our project]  <br/>
Our recommendations will help the Inter-American Development Bank identify the impoverished households and improve social inequality by providing sufficient aids to people who are in need of. At the same time, we are making sure that the bank is not wasting their precious aids to non-vulnerable people who do not need the money. Compared to the previous PMT model that the bank is using, our random forest model Although the project is primarily targeted at households in Costa Rica, the models can definitely be applied to other countries and regions in the future, and help the institution to expand its impact in improving inequality issues. 
>>>>>>> df06c11cbbe880e0e34105206b51ccecc4fe465d


##[Vision of our project]  <br/>
Our recommendations will help the Inter-American Development Bank identify the impoverished households and improve social inequality by providing sufficient aids to people who are in need of. At the same time, we are making sure that the bank is not wasting their precious aids to non-vulnerable people who do not need the money. Compared to the previous PMT model that the bank is using, our random forest model Although the project is primarily targeted at households in Costa Rica, the models can definitely be applied to other countries and regions in the future, and help the institution to expand its impact in improving inequality issues. 

